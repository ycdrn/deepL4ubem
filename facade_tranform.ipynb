{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning-Based WWR and Floor Count Extraction from FaÃ§ade Images to Improve UBEM\n",
    "\n",
    "CISBAT 2025\n",
    "\n",
    "[Ayca Duran](https://systems.arch.ethz.ch/ayca-duran), [Panagiotis Karapiperis](https://www.linkedin.com/in/panagiotis-karapiperis-ethz/), [Christoph Waibel](https://systems.arch.ethz.ch/christoph-waibel), [Arno Schlueter](https://systems.arch.ethz.ch/arno-schlueter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Facade Transformation Workflow\n",
    "\n",
    "This notebook uses the rectification.py file as introduced in [K. Chaudhury, S. DiVerdi and S. Ioffe, \"Auto-rectification of user photos\" 2014 IEEE International Conference on Image Processing (ICIP), Paris, France, 2014, pp. 3479-3483, doi: 10.1109/ICIP.2014.7025706.](https://ieeexplore.ieee.org/document/7025706) and implemented in [github repo](https://github.com/chsasank/Image-Rectification?tab=readme-ov-file) to automatically rectify images and their facade masks using homography transfomration from vanishing points estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.measure import ransac\n",
    "from rectification import *\n",
    "from tqdm import tqdm\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Facade Simplification\n",
    "\n",
    "This snipset of the notebook loads the facade predictions from the model and simplifies them into simpler shapes using Open CV approxPolyDP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the image and convert to grayscale\n",
    "image_path = \"example/images\"\n",
    "facade_predictions_path = \"example/predictions/facades_roboflow\"\n",
    "\n",
    "# Simplify Facade Predictions\n",
    "output_path_simplified_facade = \"example/predictions/facades_roboflow/facades_simplified\"\n",
    "output_path_compare_simplified_prediction = \"example/predictions/facades_roboflow/facades_simplified/figs\"\n",
    "if not os.path.exists(output_path_simplified_facade):\n",
    "    os.makedirs(output_path_simplified_facade, exist_ok=True)\n",
    "if not os.path.exists(output_path_compare_simplified_prediction):\n",
    "    os.makedirs(output_path_compare_simplified_prediction, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define output paths\n",
    "output_path_VP_masked_images = \"example/rectified/images\"\n",
    "output_path_VP_masked_annotations = \"example/rectified/facades\"\n",
    "output_path_VP_masked_rect_img_annot = \"example/rectified/facades/figs\"\n",
    "\n",
    "os.makedirs(output_path_VP_masked_images, exist_ok=True)\n",
    "os.makedirs(output_path_VP_masked_annotations, exist_ok=True)\n",
    "os.makedirs(output_path_VP_masked_rect_img_annot, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in tqdm(os.listdir(image_path)):\n",
    "    image = cv2.imread(os.path.join(image_path, filename))\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Load facade prediction & Simplify\n",
    "    facade_prediction = np.array(Image.open(os.path.join(facade_predictions_path, filename.split(\".\")[0] + \".png\")))\n",
    "    contours, _ = cv2.findContours(facade_prediction, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    epsilon = 0.015 * cv2.arcLength(contours[0], True) \n",
    "    approx_contour = cv2.approxPolyDP(contours[0], epsilon, True)\n",
    "\n",
    "    # Create a blank canvas and draw the simplified contour\n",
    "    simplified_facade_prediction = np.zeros_like(facade_prediction)\n",
    "    cv2.drawContours(simplified_facade_prediction, [approx_contour], -1, 1, -1)\n",
    "    save_filename_facade_simpl = filename.split(\".\")[0] + \"simplified.png\"\n",
    "    cv2.imwrite(os.path.join(output_path_simplified_facade, save_filename_facade_simpl), simplified_facade_prediction)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(20, 10), sharex=True)\n",
    "    axes[0].imshow(facade_prediction, cmap=\"gray\")\n",
    "    axes[0].set_title(\"Facade Prediction\")\n",
    "    axes[0].grid(True)\n",
    "    axes[1].imshow(image_rgb)\n",
    "    axes[1].imshow(simplified_facade_prediction, cmap=\"rainbow\", alpha= 0.5)\n",
    "    axes[1].set_title(\"Prediction Simplification\")\n",
    "    axes[1].grid(True)\n",
    "    plt.savefig(os.path.join(output_path_compare_simplified_prediction, filename), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    #break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Auto-Rectify\n",
    "\n",
    "This code uses a masked region of the image (bounding box of the predicted facade) to estimate vanishing points and perform homography transformation. The auto-rectified images and facade masks are saved under example/rectified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the inference images\n",
    "for filename in tqdm(os.listdir(image_path)):\n",
    "\n",
    "    image = cv2.imread(os.path.join(image_path, filename))\n",
    "    height, width, channels = image.shape\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    simplified_facade_prediction = cv2.imread(os.path.join(output_path_simplified_facade, filename.split(\".\")[0]+\"simplified.png\"), cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    ### Crop image with bounding box around facade prediction\n",
    "    y_indices, x_indices = np.where(simplified_facade_prediction > 0)\n",
    "    x_min, x_max = x_indices.min(), x_indices.max()\n",
    "    y_min, y_max = y_indices.min(), y_indices.max()\n",
    "\n",
    "    cropped_facade = image_rgb[max(0, y_min-100):min(height,y_max+1+100), max(0,x_min-100):min(width,x_max+1+100)]\n",
    "    cropped_facade_annotation = simplified_facade_prediction[max(0,y_min-100):min(height,y_max+1+100), max(0,x_min-100):min(width,x_max+1+100)]\n",
    "\n",
    "    ### HOMOGRAPHY\n",
    "    # Compute edgelets\n",
    "    edgelets1_masked = compute_edgelets(cropped_facade)\n",
    "    #vis_edgelets(cropped_facade, edgelets1_masked) # Visualize the edgelets\n",
    "    # Calculate 1st vanishing point\n",
    "    vp1_masked = ransac_vanishing_point(edgelets1_masked, num_ransac_iter=200, threshold_inlier=5)\n",
    "    vp1_masked = reestimate_model(vp1_masked, edgelets1_masked, threshold_reestimate=2)\n",
    "    #vis_model(cropped_facade, vp1_masked) # Visualize the vanishing point model\n",
    "    # Compute 2nd vanishing point\n",
    "    edgelets2_masked = remove_inliers(vp1_masked, edgelets1_masked, 10)\n",
    "    vp2_masked = ransac_vanishing_point(edgelets2_masked, num_ransac_iter=200, threshold_inlier=5)\n",
    "    vp2_masked = reestimate_model(vp2_masked, edgelets2_masked, threshold_reestimate=2)\n",
    "    #vis_model(cropped_facade, vp2_masked) # Visualize the vanishing point model\n",
    "\n",
    "    # Apply Hmography transformation\n",
    "    warped_img_masked = compute_homography_and_warp(cropped_facade, vp1_masked, vp2_masked, clip_factor=4)\n",
    "    warped_img_uint8_masked = (warped_img_masked * 255).astype(np.uint8)\n",
    "    image_to_save_masked = cv2.cvtColor(warped_img_uint8_masked, cv2.COLOR_BGR2RGB)\n",
    "    save_filename_img_masked = filename.split(\".\")[0] + \"_rectified.jpg\"\n",
    "    cv2.imwrite(os.path.join(output_path_VP_masked_images, save_filename_img_masked), image_to_save_masked)\n",
    "\n",
    "    warped_annot_masked = compute_homography_and_warp(cropped_facade_annotation, vp1_masked, vp2_masked, clip_factor=4)\n",
    "    warped_annot_uint8_masked = (warped_annot_masked * 255).astype(np.uint8)\n",
    "    save_filename_annot_masked = filename.split(\".\")[0] + \"_rectified.png\"\n",
    "    cv2.imwrite(os.path.join(output_path_VP_masked_annotations, save_filename_annot_masked), warped_annot_uint8_masked)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(20, 10), sharex=True)\n",
    "    axes[0].imshow(warped_img_uint8_masked)\n",
    "    axes[0].set_title(\"Rectified Image\")\n",
    "    axes[0].grid(False)\n",
    "    axes[1].imshow(warped_annot_uint8_masked, cmap=\"rainbow\")\n",
    "    axes[1].set_title(\"Rectified Annotation\")\n",
    "    axes[1].grid(False)\n",
    "\n",
    "    ### Find bounding box\n",
    "    y_indices, x_indices = np.where(warped_annot_masked > 0)\n",
    "    x_min, x_max = x_indices.min(), x_indices.max()\n",
    "    y_min, y_max = y_indices.min(), y_indices.max()\n",
    "    # visualize bounding box\n",
    "    rgb_mask_masked = np.stack([warped_annot_masked] * 3, axis=-1) * 255  # Convert binary to RGB\n",
    "    cv2.rectangle(rgb_mask_masked, (x_min, y_min), (x_max, y_max), color=(0, 255, 0), thickness=7)\n",
    "    axes[2].imshow(warped_img_uint8_masked)\n",
    "    axes[2].imshow(rgb_mask_masked, alpha=0.5)\n",
    "    axes[2].set_title(\"Bounding Box\")\n",
    "    axes[2].grid(False)\n",
    "    plt.savefig(os.path.join(output_path_VP_masked_rect_img_annot, save_filename_img_masked),dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    #break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
