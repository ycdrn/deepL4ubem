{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning-Based WWR and Floor Count Extraction from FaÃ§ade Images to Improve UBEM\n",
    "\n",
    "CISBAT 2025\n",
    "\n",
    "[Ayca Duran](https://systems.arch.ethz.ch/ayca-duran), [Panagiotis Karapiperis](https://www.linkedin.com/in/panagiotis-karapiperis-ethz/), [Christoph Waibel](https://systems.arch.ethz.ch/christoph-waibel), [Arno Schlueter](https://systems.arch.ethz.ch/arno-schlueter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FCN Windows Finder - Pretrained on COCO with VOC labels\n",
    "\n",
    "This notebook performs inference of the FCN model (ResNet50) on the rectified images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torchvision.models.segmentation import fcn_resnet50\n",
    "\n",
    "# CPU / GPU\n",
    "device = torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Images Prep\n",
    "\n",
    "This part of the notebook reads images from the custom inference folder (under example) and prepares them for the trained fcn model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_path = \"example/rectified/images\"\n",
    "print(len(os.listdir(images_path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations for input to the model\n",
    "to_tensor = transforms.Compose([transforms.Resize((520, 520)), transforms.PILToTensor()])\n",
    "norm_img = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Loads the pretrained model from the hub, and loads the weights (output of the fcn_training.ipynb notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.segmentation import fcn_resnet50\n",
    "\n",
    "# Define model name\n",
    "model_name = \"fcn_resnet50\" \n",
    "trained_model_path = f\"trained_models/{model_name}/model_best.pt\"\n",
    "\n",
    "# Load model, adjust classifier and load weights\n",
    "model = fcn_resnet50(weights='COCO_WITH_VOC_LABELS_V1').to(device)\n",
    "model.classifier[-1] = nn.Conv2d(model.classifier[4].in_channels,1,kernel_size=(1,1),stride=(1,1)).to(device)\n",
    "model.aux_classifier[-1] = nn.Conv2d(model.aux_classifier[4].in_channels,1, kernel_size=(1,1),stride=(1,1)).to(device)\n",
    "model.load_state_dict(torch.load(trained_model_path))\n",
    "\n",
    "model.eval()\n",
    "model.to(device)\n",
    "print(f\"{model_name} loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save path\n",
    "save_path = f\"example/predictions/{model_name}_rectified\"\n",
    "os.makedirs(save_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loop Inference\n",
    "\n",
    "This part of the notebook iterates over the defined inference images set, and predicts window masks using the trained model (FCN-ResNet50)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate\n",
    "for index, filename in tqdm(enumerate(os.listdir(images_path))):\n",
    "\n",
    "  # Open image\n",
    "  image_path = os.path.join(images_path, filename)\n",
    "  orig_image = Image.open(image_path).convert('RGB')\n",
    "  size = orig_image.size\n",
    "  image = to_tensor(orig_image)/255\n",
    "  img = norm_img(image.type(torch.float))\n",
    "\n",
    "  # get prediction of windows\n",
    "  with torch.no_grad():\n",
    "    prediction = model(img.to(device).unsqueeze(dim=0))['out']\n",
    "    pred_soft = torch.nn.functional.sigmoid(prediction)\n",
    "    pred_binary = (pred_soft > 0.5).squeeze().cpu().numpy() # set threshold\n",
    "  \n",
    "  mask = pred_binary.astype(int)\n",
    "  mask = cv2.resize(mask, size, interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "  cv2.imwrite(os.path.join(save_path, filename.split(\".jpg\")[0]+\".png\"), mask)\n",
    "  \n",
    "  # Optional: Plot\n",
    "  \n",
    "  fig, axes = plt.subplots(1,2,figsize=(12,5))\n",
    "  axes[0].imshow(orig_image)\n",
    "  axes[0].set_title(f'Original Image {filename}')\n",
    "  axes[1].imshow(mask, cmap=\"grey\")\n",
    "  axes[1].set_title('Windows Mask')\n",
    "  for ax in axes:\n",
    "      ax.axis(False)\n",
    "  #break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
