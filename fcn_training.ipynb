{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning-Based WWR and Floor Count Extraction from FaÃ§ade Images to Improve UBEM\n",
    "\n",
    "CISBAT 2025\n",
    "\n",
    "[Ayca Duran](https://systems.arch.ethz.ch/ayca-duran), [Panagiotis Karapiperis](https://www.linkedin.com/in/panagiotis-karapiperis-ethz/), [Christoph Waibel](https://systems.arch.ethz.ch/christoph-waibel), [Arno Schlueter](https://systems.arch.ethz.ch/arno-schlueter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FCN Windows Finder - Pretrained on COCO with VOC labels\n",
    "\n",
    "This notebook is to train an FCN model for binary semantic segmentation to parse windows on facades, using the zju_facade_jcst2020 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_pOTAOdcXCHx"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "# For reproducibility\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# CPU / GPU\n",
    "device = torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset Class\n",
    "class ImageAnnotationDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, target_size=(520, 520)):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_dir (str): Directory with input RGB images.\n",
    "            mask_dir (str): Directory with binary PNG masks.\n",
    "            target_size (tuple): Image/mask resize dimensions (H, W).\n",
    "        \"\"\"\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.target_size = target_size\n",
    "\n",
    "        self.image_filenames = sorted([\n",
    "            f for f in os.listdir(image_dir) if f.endswith(('.jpg', '.png'))\n",
    "        ])\n",
    "\n",
    "        self.to_tensor = transforms.PILToTensor()\n",
    "        self.resize = transforms.Resize(target_size)\n",
    "        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                              std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filename = self.image_filenames[idx]\n",
    "        base_name = os.path.splitext(filename)[0]\n",
    "        image_path = os.path.join(self.image_dir, filename)\n",
    "        mask_path = os.path.join(self.mask_dir, base_name + \".png\")\n",
    "\n",
    "        # Load and process image\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        image = self.resize(image)\n",
    "        image = self.to_tensor(image).float() / 255.0\n",
    "        image = self.normalize(image)\n",
    "\n",
    "        # Load and process mask\n",
    "        mask = Image.open(mask_path).convert(\"L\")  # ensure grayscale\n",
    "        mask = self.resize(mask)\n",
    "        mask = self.to_tensor(mask)[0]  # single channel\n",
    "\n",
    "        return image, mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Download the dataset from [Li et al., 2020](https://github.com/lck1201/win_det_heatmaps) and place it within the root folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set paths for full zju_facade_jcst2020 dataset\n",
    "dataset_dir = 'zju_facade_jcst2020'\n",
    "\n",
    "train_image_path = os.path.join(dataset_dir, 'train', 'images')\n",
    "val_image_path = os.path.join(dataset_dir, 'test', 'images')\n",
    "\n",
    "train_ann_path = os.path.join(dataset_dir, 'train', 'annotations')\n",
    "val_ann_path = os.path.join(dataset_dir, 'test', 'annotations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset objects\n",
    "train_dataset = ImageAnnotationDataset(train_image_path, train_ann_path)\n",
    "val_dataset = ImageAnnotationDataset(val_image_path, val_ann_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Sbj-3DBb6hZ"
   },
   "outputs": [],
   "source": [
    "# Dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "We use a pretrained ResNet50 model on the COCO dataset with VOC labels. We load the model from torchvision models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l36IpkLm7EgA",
    "outputId": "c00669d2-651c-4b0d-e36f-1591329ef7cb"
   },
   "outputs": [],
   "source": [
    "# Import model and modify last layer for binary segmentation\n",
    "from torchvision.models.segmentation import fcn_resnet50\n",
    "model = fcn_resnet50(weights='COCO_WITH_VOC_LABELS_V1').to(device)\n",
    "model.classifier[-1] = nn.Conv2d(model.classifier[4].in_channels,1,kernel_size=(1,1),stride=(1,1)).to(device)\n",
    "model.aux_classifier[-1] = nn.Conv2d(model.aux_classifier[4].in_channels,1, kernel_size=(1,1),stride=(1,1)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hRupBmp3gln7",
    "outputId": "2185e2c2-e8d8-4955-f524-c84e80292c27"
   },
   "outputs": [],
   "source": [
    "list(model.children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Training / Evaluation steps\n",
    "import evaluate\n",
    "from torch.nn.functional import binary_cross_entropy_with_logits\n",
    "\n",
    "# Metric setup\n",
    "iou_metric = evaluate.load(\"mean_iou\")\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for images, masks in tqdm(dataloader, desc=\"Training\", leave=False):\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device).float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)['out']\n",
    "        outputs = outputs.squeeze(1)\n",
    "\n",
    "        loss = torch.nn.functional.binary_cross_entropy_with_logits(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_one_epoch(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for images, masks in tqdm(dataloader):\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device).float()\n",
    "\n",
    "        outputs = model(images)['out'].squeeze(1)  # (B, H, W)\n",
    "\n",
    "        loss = binary_cross_entropy_with_logits(outputs, masks)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        probs = torch.sigmoid(outputs)\n",
    "        preds = (probs > 0.5).long()\n",
    "\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(masks.cpu().numpy().astype(int))\n",
    "\n",
    "    # Compute IoU using Hugging Face's evaluate\n",
    "    metrics = iou_metric.compute(\n",
    "        predictions=all_preds,\n",
    "        references=all_labels,\n",
    "        num_labels=2,\n",
    "        ignore_index=None\n",
    "    )\n",
    "    iou_class_1 = metrics[\"per_category_iou\"][1]  # class 1\n",
    "\n",
    "    return total_loss / len(dataloader), iou_class_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "from torch import optim\n",
    "\n",
    "checkpoint_dir = 'trained_models/fcn_resnet50'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "num_epochs = 10 # Adjust\n",
    "lr = 0.0005\n",
    "\n",
    "def create_optimizer(net, lr):\n",
    "    # Create optimizer\n",
    "    opt = optim.Adam(net.parameters(), lr=lr)\n",
    "    return opt\n",
    "\n",
    "optimizer = create_optimizer(model, lr)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_ious = [0]\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, device)\n",
    "    train_losses.append(train_loss)\n",
    "    val_loss, val_iou = eval_one_epoch(model, val_loader, device)\n",
    "    val_losses.append(val_loss)\n",
    "    # Save models based on evaluation IoU\n",
    "    if val_iou > max(val_ious):\n",
    "       checkpoint_path = os.path.join(checkpoint_dir, f\"model_best.pt\")\n",
    "       torch.save(model.state_dict(), checkpoint_path)\n",
    "    val_ious.append(val_iou)\n",
    "\n",
    "    print(f\"[Epoch {epoch+1}] Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val IoU (class 1): {val_iou:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "wmJi92yXS0a_",
    "outputId": "c6d6b720-73c4-4841-b79e-a56229f48ce0"
   },
   "outputs": [],
   "source": [
    "# Plot validation loss during training\n",
    "xs = [x for x in range(len(val_ious))]\n",
    "plt.plot(xs, val_ious)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"IoU\")\n",
    "plt.title(\"Validation IoUs\")\n",
    "plt.savefig('valid_ious.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
